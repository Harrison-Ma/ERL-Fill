# import SysOut
from CommonInterface.modbus_slave import modbus_slave_client
from VirtualWeightController import VirtualWeightController

from torch.utils.tensorboard import SummaryWriter

import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer

import numpy as np
import random
import time
from pymodbus.client import ModbusSerialClient

import torch
import torch.nn as nn
import torch.optim as optim

from collections import deque

from tqdm import tqdm  # âœ… æ·»åŠ è¿›åº¦æ¡æ”¯æŒ

import platform

import logging
import os

from CommonInterface.Logger import init_logger

from EmotionModule import EmotionModuleNone


class Actor(nn.Module):
    def __init__(self, state_dim, action_dim, param_bounds, emotion_dim=3, hidden_size=128, dropout_rate=0.2):
        super(Actor, self).__init__()
        self.dropout_rate = dropout_rate

        # è¾“å…¥å±‚ï¼ˆå«æƒ…æ„Ÿç»´åº¦ï¼‰
        self.input_layer = nn.Linear(state_dim + emotion_dim, hidden_size)

        # æ·±å±‚ç½‘ç»œç»“æ„
        self.hidden = nn.Sequential(
            nn.LeakyReLU(0.1),
            nn.Dropout(p=dropout_rate),
            nn.Linear(hidden_size, hidden_size),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Linear(hidden_size // 2, action_dim),
            nn.Softsign()  # è¾“å‡ºæ§åˆ¶åœ¨ (-1, 1)
        )

        # æƒ…æ„Ÿæ‰°åŠ¨æƒé‡
        self.emotion_weights = nn.Parameter(torch.randn(emotion_dim, action_dim) * 0.2)

        # å‚æ•°ç¼©æ”¾å’Œåç§»ï¼ˆåŠ¨ä½œæ˜ å°„ï¼‰
        self._init_param_scaling(param_bounds)

    def _init_param_scaling(self, param_bounds):
        scale_params, offset_params = [], []
        for key in param_bounds.keys():
            low, high = param_bounds[key]
            scale = (high - low) / 2.0
            offset = (high + low) / 2.0
            scale_params.append(scale)
            offset_params.append(offset)
        self.register_buffer('scale_params', torch.tensor(scale_params, dtype=torch.float32))
        self.register_buffer('offset_params', torch.tensor(offset_params, dtype=torch.float32))

    def forward(self, state, emotion):
        if emotion.dim() == 1:
            emotion = emotion.unsqueeze(0)
        if state.dim() == 1:
            state = state.unsqueeze(0)
        if emotion.size(0) != state.size(0):
            emotion = emotion.expand(state.size(0), -1)

        x = torch.cat([state, emotion], dim=1)
        x = self.input_layer(x)
        base_action = self.hidden(x)  # [-1, 1]

        # === æƒ…ç»ªå› å­è§£æ„ ===
        curiosity = emotion[:, 0:1]      # æ¢ç´¢
        conserv   = emotion[:, 1:2]      # ä¿å®ˆ
        anxiety   = emotion[:, 2:3]      # ç„¦è™‘

        # === åŸå§‹æ‰°åŠ¨è®¡ç®— ===
        raw_effect = torch.tanh((emotion @ self.emotion_weights) * 1.5)  # [-1, 1]

        # === è®¡ç®— alpha ç³»æ•° ===
        alpha = 0.1 + 0.9 * anxiety                  # åŸºç¡€éšç„¦è™‘å¢å¼º
        alpha *= (1.0 - 0.5 * conserv)               # ä¿å®ˆæ€§å‰Šå¼±å¹…åº¦
        alpha *= (1.0 + 0.5 * curiosity)             # æ¢ç´¢å¢å¼ºè°ƒæ•´å¼¹æ€§
        alpha = torch.clamp(alpha, min=0.01, max=1)  # é™åˆ¶æ‰°åŠ¨èŒƒå›´

        # === è°ƒæ•´æ‰°åŠ¨æ–¹å‘ï¼ˆé¼“åŠ±é€ƒç¦»é¥±å’ŒåŒºï¼‰ ===
        modulator = (1 - torch.abs(base_action)) ** 2
        sign_flip = -torch.sign(base_action)  # å¦‚æœåœ¨è¾¹ç•Œé™„è¿‘ï¼Œå°±å‘ä¸­å¿ƒåå‘æ‰°åŠ¨
        adjusted_emotion_effect = alpha * raw_effect * sign_flip * modulator  # [-Î±, Î±]

        # === æœ€ç»ˆåŠ¨ä½œè®¡ç®— ===
        raw_action = base_action + adjusted_emotion_effect
        scale_params = self.scale_params.to(raw_action.device)
        offset_params = self.offset_params.to(raw_action.device)
        scaled_action = raw_action * scale_params + offset_params

        return scaled_action


# class Critic(nn.Module):
#     def __init__(self, state_dim, action_dim, emotion_dim=3, hidden_size=128):
#         super(Critic, self).__init__()
#         # è¾“å…¥å±‚å¢åŠ æƒ…æ„Ÿç»´åº¦
#         self.net = nn.Sequential(
#             nn.Linear(state_dim + action_dim + emotion_dim, hidden_size),
#             nn.ReLU(),
#             nn.Linear(hidden_size, hidden_size),
#             nn.ReLU(),
#             nn.Linear(hidden_size, 1)
#         )
#
#     def forward(self, state, action, emotion):
#         return self.net(torch.cat([state, action, emotion], 1))

class Critic(nn.Module):
    def __init__(self, state_dim, action_dim, emotion_dim=3, hidden_size=256, dropout_rate=0.2):
        super(Critic, self).__init__()
        input_dim = state_dim + action_dim + emotion_dim

        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.LeakyReLU(0.1),
            nn.Dropout(p=dropout_rate),

            nn.Linear(hidden_size, hidden_size),
            nn.LayerNorm(hidden_size),
            nn.ReLU(),
            nn.Dropout(p=dropout_rate),

            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),

            nn.Linear(hidden_size // 2, 1)
        )

    def forward(self, state, action, emotion):
        # æƒ…æ„Ÿæ ‡å‡†åŒ–ï¼šæŠ‘åˆ¶æƒ…æ„Ÿæ¼‚ç§»å¸¦æ¥çš„ Q ä¼°è®¡ä¸ç¨³å®š
        emotion = (emotion - emotion.mean(dim=0, keepdim=True)) / (emotion.std(dim=0, keepdim=True) + 1e-6)

        x = torch.cat([state, action, emotion], dim=1)
        q = self.net(x)
        return torch.clamp(q, -1e6, 1e6)

class PrioritizedReplayBuffer:
    def __init__(self, capacity=100000, alpha=0.6, epsilon=1e-4):
        self.capacity = capacity
        self.alpha = alpha
        self.epsilon = epsilon
        self.buffer = []
        self.priorities = []

    def add(self, transition, priority):
        """æ·»åŠ ç»éªŒå¹¶è®¡ç®—å¯¹åº”çš„ä¼˜å…ˆçº§"""
        p = (abs(priority) + self.epsilon) ** self.alpha
        p = np.nan_to_num(p, nan=self.epsilon)  # âœ… é˜²æ­¢å‡ºç° NaN

        if len(self.buffer) < self.capacity:
            self.buffer.append(transition)
            self.priorities.append(p)
        else:
            idx = np.argmin(self.priorities)  # âœ… æ›¿æ¢ä¼˜å…ˆçº§æœ€ä½çš„
            self.buffer[idx] = transition
            self.priorities[idx] = p

    def sample(self, batch_size):
        """æ ¹æ®æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·"""
        priorities = np.array(self.priorities, dtype=np.float32)

        # âœ… é˜²æ­¢ priorities.sum() ä¸º 0 æˆ– NaN
        total = priorities.sum()
        if total <= 0 or np.isnan(total):
            priorities += self.epsilon
            total = priorities.sum()

        probs = priorities / total
        probs = np.nan_to_num(probs, nan=1.0 / len(probs))  # âœ… é˜²æ­¢å‡ºç° NaN

        indices = np.random.choice(len(self.buffer), batch_size, p=probs)
        return [self.buffer[i] for i in indices]

    def __len__(self):
        return len(self.buffer)

class DDPGAgent:
    def __init__(self, env, device='cuda' if torch.cuda.is_available() else 'cpu', use_td_error=False):
        param_bounds = env.bounds
        self.env = env
        self.device = device
        self.use_td_error = use_td_error

        self.emotion = EmotionModuleNone()

        self.actor = Actor(env.state_dim, env.action_dim, param_bounds).to(device)
        self.actor_target = Actor(env.state_dim, env.action_dim, param_bounds).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())

        self.critic = Critic(env.state_dim, env.action_dim).to(device)
        self.critic_target = Critic(env.state_dim, env.action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())

        self.gamma = 0.99
        self.tau = 0.01
        self.batch_size = 128
        self.memory_size = 100000
        self.actor_lr = 1e-4
        self.critic_lr = 1e-3

        self.train_step = 1  # æˆ– self.episode_countï¼Œå¦‚æœä»¥è½®æ¬¡ä¸ºåŸºç¡€
        self.lr_decay_rate = 0.999  # æ¯æ­¥æˆ–æ¯è½®è¡°å‡ 0.5%

        self.target_update_freq = 1  # æ¯ 2 æ­¥æ›´æ–°ä¸€æ¬¡
        self.learn_step = 0

        self.actor_update_freq = 1  # æ¯éš” 2 æ¬¡ Critic å­¦ä¹ æ›´æ–°ä¸€æ¬¡ Actor

        self.actor_optim = optim.Adam(self.actor.parameters(), lr=self.actor_lr)
        self.critic_optim = optim.Adam(self.critic.parameters(), lr=self.critic_lr)

        # self.memory = []  # åˆ‡æ¢ä¸ºåˆ—è¡¨ä»¥æ”¯æŒTD-errorå­˜å‚¨
        self.memory = PrioritizedReplayBuffer(capacity=self.memory_size)
        self.ou_noise = OUNoise(env.action_dim)

    def act(self, state, add_noise=True):
        emotion_state = self.emotion.get_emotion()

        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)
        emotion_tensor = torch.from_numpy(emotion_state).float().unsqueeze(0).to(self.device)

        with torch.no_grad():
            scaled_action = self.actor(state_tensor, emotion_tensor)

        if not add_noise:
            return scaled_action.cpu().numpy().flatten()

        noise = self.ou_noise.sample()
        scale = self.actor.scale_params.cpu().numpy()
        offset = self.actor.offset_params.cpu().numpy()

        final_action = np.clip(
            scaled_action.cpu().numpy().flatten() + noise * scale,
            offset - scale,
            offset + scale
        )
        return final_action

    def remember(self, state, action, reward, next_state, done):
        emotion_state = self.emotion.get_emotion()

        action = np.array(action)
        norm_action = (action - self.actor.offset_params.cpu().numpy()) / self.actor.scale_params.cpu().numpy()
        norm_action = np.clip(norm_action, -1.0, 1.0)

        td_error = 0
        if self.use_td_error:
            with torch.no_grad():
                state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
                next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)
                action_tensor = torch.FloatTensor(norm_action).unsqueeze(0).to(self.device)
                emotion_tensor = torch.FloatTensor(emotion_state).unsqueeze(0).to(self.device)

                next_emotion_tensor = torch.FloatTensor(self.emotion.get_emotion()).unsqueeze(0).to(self.device)
                next_action = self.actor_target(next_state_tensor, next_emotion_tensor)
                target_q = self.critic_target(next_state_tensor, next_action, next_emotion_tensor)
                expected_q = reward + (1 - int(done)) * self.gamma * target_q.item()
                current_q = self.critic(state_tensor, action_tensor, emotion_tensor).item()
                td_error = abs(expected_q - current_q)

        # self.memory.append((state, norm_action, reward, next_state, done, emotion_state, td_error))

        # if len(self.memory) > self.memory_size:
        #     self.memory.pop(0)
        self.memory.add(
            transition=(state, norm_action, reward, next_state, done, emotion_state, td_error),
            priority=td_error  # ä½¿ç”¨ TD-error åˆå§‹åŒ–ä¼˜å…ˆçº§
        )

    def compute_dynamic_lr(emotion, base_lr, role="critic", min_lr=5e-5, max_lr=2e-3):
        anxiety, conservativeness, curiosity = emotion

        if role == "critic":
            factor = 1.0 - 0.5 * anxiety + 0.4 * conservativeness + 0.1 * curiosity
        elif role == "actor":
            factor = 1.0 + 0.4 * curiosity - 0.3 * conservativeness
        else:
            factor = 1.0

        return float(np.clip(base_lr * factor, min_lr, max_lr))

    def learn(self):
        if len(self.memory) < self.batch_size:
            return

        # ----- 1. é‡‡æ ·ç­–ç•¥ -----
        # if self.use_td_error:
        #     td_errors = np.array([m[6] for m in self.memory]) + 1e-6
        #     prob = td_errors / td_errors.sum()
        #     indices = np.random.choice(len(self.memory), self.batch_size, p=prob)
        #     batch = [self.memory[i] for i in indices]
        # else:
        #     batch = random.sample(self.memory, self.batch_size)
        if self.use_td_error:
            batch = self.memory.sample(self.batch_size)
        else:
            batch = random.sample(self.memory.buffer, self.batch_size)

        # ----- 2. è§£åŒ… & Tensor è½¬æ¢ -----
        states, actions, rewards, next_states, dones, emotions, _ = zip(*batch)

        states = torch.FloatTensor(states).to(self.device)
        actions = torch.FloatTensor(actions).to(self.device)
        rewards = torch.FloatTensor(rewards).unsqueeze(1).to(self.device)
        next_states = torch.FloatTensor(next_states).to(self.device)
        dones = torch.FloatTensor(dones).unsqueeze(1).to(self.device)
        emotions = torch.FloatTensor(emotions).to(self.device)

        # âœ… reward ç¼©æ”¾
        reward_scale = 0.01
        rewards = rewards * reward_scale

        # âœ… è·å–å½“å‰å¹³å‡æƒ…æ„ŸæŒ‡æ ‡
        anxiety = torch.mean(emotions[:, 2]).item()
        conservativeness = torch.mean(emotions[:, 1]).item()
        curiosity = torch.mean(emotions[:, 0]).item()

        actor_factor = 1.0 + 0.8 * curiosity - 0.4 * conservativeness + 0.2 * anxiety
        critic_factor = 1.0 - 0.3 * anxiety + 0.6 * conservativeness
        # âœ… èåˆæƒ…æ„Ÿè°ƒèŠ‚æœºåˆ¶ï¼ŒåŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡
        # critic_factor = 1.0 - 0.5 * anxiety + 0.4 * conservativeness + 0.1 * curiosity
        # actor_factor = 1.0 + 0.4 * curiosity - 0.3 * conservativeness

        # critic_scaled_lr = float(np.clip(self.critic_lr * critic_factor, self.critic_lr*0.5, self.critic_lr*5))
        # actor_scaled_lr = float(np.clip(self.actor_lr * actor_factor, self.actor_lr*0.5, self.actor_lr*5))

        # âœ… åŠ å…¥ step è¡°å‡é¡¹ï¼ˆæ¯æ¬¡å­¦ä¹ é€æ­¥è¡°å‡ï¼‰
        decay_factor = self.lr_decay_rate ** self.train_step  # è¶Šå¾€åè¶Šå°

        critic_scaled_lr = float(np.clip(self.critic_lr * critic_factor * decay_factor,
                                         self.critic_lr * 0.1, self.critic_lr * 3))
        actor_scaled_lr = float(np.clip(self.actor_lr * actor_factor * decay_factor,
                                        self.actor_lr * 0.1, self.actor_lr * 3))

        # print("critic_scaled_lr:",critic_scaled_lr)
        # âœ… åº”ç”¨åˆ°ä¼˜åŒ–å™¨
        for g in self.critic_optim.param_groups:
            g["lr"] = critic_scaled_lr
        for g in self.actor_optim.param_groups:
            g["lr"] = actor_scaled_lr

        # ----- 3. Critic æ›´æ–° -----
        with torch.no_grad():
            next_emotion_tensor = torch.FloatTensor(self.emotion.get_emotion()).unsqueeze(0).to(self.device)
            next_emotion_tensor = next_emotion_tensor.expand(next_states.size(0), -1)
            next_actions = self.actor_target(next_states, next_emotion_tensor)
            target_q = self.critic_target(next_states, next_actions, next_emotion_tensor)
            target_q = rewards + (1 - dones) * self.gamma * target_q

        current_q = self.critic(states, actions, emotions)
        critic_loss = nn.SmoothL1Loss()(current_q, target_q)

        self.critic_optim.zero_grad()
        critic_loss.backward()
        nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=5.0)
        self.critic_optim.step()

        # ----- 4. Actor æ›´æ–° -----
        actor_loss = None  # å…ˆå ä½

        if self.learn_step % self.actor_update_freq == 0:
            action_out = self.actor(states, emotions)

            # ğŸ”¥ åŸå§‹ lossï¼šæœ€å¤§åŒ– Q å€¼
            actor_loss = -self.critic(states, action_out, emotions).mean()

            # âœ… å¤šæ ·æ€§æ­£åˆ™é¡¹ï¼ˆé¼“åŠ± std è¶Šå¤§è¶Šå¥½ï¼‰
            diversity_term = torch.std(action_out, dim=1).mean()
            actor_loss -= 0.03 * diversity_term  # åŠ¨ä½œå¤šæ ·æ€§æ¿€åŠ±

            # âœ… L2 æ­£åˆ™é¡¹ï¼ˆé¼“åŠ±æƒé‡ä¸è¦è¿‡å¤§ï¼‰
            l2_lambda = 1e-4  # æ§åˆ¶æ­£åˆ™å¼ºåº¦ï¼ˆå¯è°ƒï¼‰
            l2_reg = torch.tensor(0., device=self.device)
            for param in self.actor.parameters():
                l2_reg += torch.norm(param, p=2)
            actor_loss += l2_lambda * l2_reg  # åŠ åˆ°æœ€ç»ˆ loss ä¸­

            # âœ… åå‘ä¼ æ’­ + æ¢¯åº¦è£å‰ª
            self.actor_optim.zero_grad()
            actor_loss.backward()
            nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=5.0)
            self.actor_optim.step()

        # ----- 5. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ -----
        self.learn_step += 1  # âœ… æ­¥æ•°é€’å¢
        # self.train_step += 1
        #
        # print("self.train_step:",self.train_step)

        if self.learn_step % self.target_update_freq == 0:
            for target_param, param in zip(self.actor_target.parameters(), self.actor.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

            for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
                target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)

        # logger.debug(
        #     f"[Anxiety: {anxiety:.2f}] Critic LR: {critic_scaled_lr:.6f}, Actor LR: {actor_scaled_lr:.6f} | "
        #     f"Losses - Critic: {getattr(critic_loss, 'item', lambda: 'None')()}, "
        #     f"Actor: {getattr(actor_loss, 'item', lambda: 'None')()}"
        # )

        # self.memory.add(transition, new_priority)
        return {"critic_loss": critic_loss.item() if critic_loss is not None else None,
                "actor_loss": actor_loss.item() if actor_loss is not None else None}

    def load_weights(self, weights):
        self.actor.load_state_dict(weights['actor'])
        self.critic.load_state_dict(weights['critic'])

    def get_weights(self):
        return {
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict()
        }

    # def save(self, filename):
    #     torch.save({
    #         'actor': self.actor.state_dict(),
    #         'critic': self.critic.state_dict(),
    #         'emotion': self.emotion.current_emotion,
    #         'emotion_transformer': self.emotion.transformer.state_dict()  # âœ… æ–°å¢éƒ¨åˆ†
    #     }, filename)

    def save(self, path):
        save_dict = {
            'actor': self.actor.state_dict(),
            'critic': self.critic.state_dict(),
            'actor_target': self.actor_target.state_dict(),
            'critic_target': self.critic_target.state_dict(),
            'actor_optim': self.actor_optim.state_dict(),
            'critic_optim': self.critic_optim.state_dict(),
            'train_step': self.train_step,
        }

        # âœ… å¯é€‰ä¿å­˜ emotion transformer
        if hasattr(self, "emotion") and hasattr(self.emotion, "transformer"):
            save_dict['emotion_transformer'] = self.emotion.transformer.state_dict()

        torch.save(save_dict, path)
        # logger.info(f"æ¨¡å‹å·²ä¿å­˜è‡³ {path}")

    def load(self, filename):
        checkpoint = torch.load(filename, map_location=self.device,weights_only=False)
        # checkpoint = torch.load(filename, map_location=self.device)

        self.actor.load_state_dict(checkpoint['actor'])
        self.critic.load_state_dict(checkpoint['critic'])

        # âœ… å®‰å…¨åŠ è½½ current_emotion
        if 'emotion' in checkpoint:
            self.emotion.current_emotion = checkpoint['emotion']
        else:
            print("âš ï¸ æœªå‘ç° emotion çŠ¶æ€å­—æ®µï¼Œè·³è¿‡ current_emotion åŠ è½½")

        # âœ… å®‰å…¨åŠ è½½ EmotionTransformerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'emotion_transformer' in checkpoint:
            if hasattr(self.emotion, 'transformer'):
                self.emotion.transformer.load_state_dict(checkpoint['emotion_transformer'])
                print("âœ… EmotionTransformer æƒé‡å·²æˆåŠŸåŠ è½½")
            else:
                print("âš ï¸ æ£€æŸ¥ç‚¹åŒ…å« EmotionTransformer æƒé‡ï¼Œä½†å½“å‰ emotion æ¨¡å—ä¸æ”¯æŒ transformerï¼Œå·²è·³è¿‡åŠ è½½")
        else:
            print("âš ï¸ æœªå‘ç° EmotionTransformer æƒé‡å­—æ®µï¼Œè·³è¿‡åŠ è½½")

import shutil  # ç¡®ä¿å¯¼å…¥

def reset_actor_scaling(agent, new_bounds):
    """
    é‡ç½® Actor å’Œ Actor-Target çš„å‚æ•°ç¼©æ”¾ä¿¡æ¯ï¼ˆscale_params å’Œ offset_paramsï¼‰

    å‚æ•°:
        agent: DDPGAgent å®ä¾‹
        new_bounds: dict, ä¾‹å¦‚ env.bounds
    """
    if hasattr(agent.actor, '_init_param_scaling'):
        agent.actor._init_param_scaling(new_bounds)
    if hasattr(agent.actor_target, '_init_param_scaling'):
        agent.actor_target._init_param_scaling(new_bounds)
    print("âœ… Actor ç¼©æ”¾å‚æ•°å·²æ ¹æ®æ–° bounds é‡æ–°åˆå§‹åŒ–ã€‚")

def train_ddpg(env, agent, episodes=1000, max_steps=500, log_prefix="ddpg_emotion", logger=None):
    if logger is None:
        logger = init_logger(log_prefix=log_prefix)
    rewards = []
    env.max_steps = max_steps

    tb_log_dir = f"runs/{log_prefix}"
    if os.path.exists(tb_log_dir):
        shutil.rmtree(tb_log_dir)
    writer = SummaryWriter(log_dir=tb_log_dir)

    global_step = 0
    pretrain_path = "pre_train_pth/AIM_v1_sim_24900_25000.pth"
    use_pretrain = 0
    if not use_pretrain:
        logger.info("å·²é€‰æ‹©ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        print("å·²é€‰æ‹©ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
    elif os.path.exists(pretrain_path):
        weights = torch.load(pretrain_path, weights_only=False)
        # weights = torch.load(pretrain_path)
        agent.load_weights(weights)
        reset_actor_scaling(agent, env.bounds)
        logger.info(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
        print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
    else:
        logger.info("æœªå‘ç°é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
        print("æœªå‘ç°é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")

    start_100 = time.time()  # â±ï¸ åˆå§‹åŒ–100å›åˆè®¡æ—¶å™¨

    for ep in tqdm(range(episodes), desc="Training Progress", ncols=100):
        state = env.reset()
        episode_reward = 0
        agent.ou_noise.reset()
        agent.ou_noise.anneal()
        average_step = 0

        for step in range(max_steps):
            action = np.transpose(agent.act(state))
            next_state, reward, done, info = env.step(action)
            movement = next_state - state
            agent.emotion.update(reward, movement)
            agent.remember(state, action, reward, next_state, done)
            state = next_state
            episode_reward += reward

            loss = agent.learn()
            if loss:
                if loss.get('actor_loss') is not None:
                    writer.add_scalar("Loss/Actor", loss['actor_loss'], global_step)
                if loss.get('critic_loss') is not None:
                    writer.add_scalar("Loss/Critic", loss['critic_loss'], global_step)
                writer.add_scalar("LR/Actor", agent.actor_optim.param_groups[0]['lr'], global_step)
                writer.add_scalar("LR/Critic", agent.critic_optim.param_groups[0]['lr'], global_step)

            cur_emotion = agent.emotion.get_emotion()
            if isinstance(cur_emotion, torch.Tensor):
                cur_emotion = cur_emotion.detach().cpu().numpy().squeeze()
            cur_emotion = np.array(cur_emotion).flatten()
            writer.add_scalar("Emotion/Curiosity", float(cur_emotion[0]), global_step)
            writer.add_scalar("Emotion/Conservativeness", float(cur_emotion[1]), global_step)
            writer.add_scalar("Emotion/Anxiety", float(cur_emotion[2]), global_step)

            if step % (max_steps // 10) == 0 or step == max_steps - 1:
                logger.info(
                    f"Ep {ep:03d} Step {step:03d} | Reward: {reward:.2f} | Loss: {loss} | Emotion: {np.round(cur_emotion, 2)}"
                )

            global_step += 1
            average_step += 1
            if done:
                print(f"âœ”ï¸ Episode {ep} finished early at step {step}")
                break

        rewards.append(episode_reward)
        writer.add_scalar("Reward/Episode", episode_reward, ep)
        writer.add_scalar("Reward/Episode(average)", episode_reward / average_step, ep)
        writer.add_scalar("Metric/Weight", info["final_weight"], ep)
        writer.add_scalar("Metric/Error", info["weight_error"], ep)
        writer.add_scalar("Metric/Time", info["total_time"], ep)
        writer.add_scalar("Reward/Step", reward, ep)

        for key, value in info["action"].items():
            writer.add_scalar(f"Action/{key}", value, ep)

        writer.add_scalar("Metric/SlowWeight", info["action"].get("slow_weight", 0.0), ep)

        if ep % 1 == 0 or ep == episodes - 1:
            cur_emotion = agent.emotion.get_emotion()
            if isinstance(cur_emotion, torch.Tensor):
                cur_emotion = cur_emotion.detach().cpu().numpy().squeeze()
            cur_emotion = np.array(cur_emotion).flatten()

            actor_loss = f"{loss.get('actor_loss'):.4f}" if loss and loss.get("actor_loss") is not None else "-"
            critic_loss = f"{loss.get('critic_loss'):.4f}" if loss and loss.get("critic_loss") is not None else "-"
            slow_weight = info["action"].get("slow_weight", 0.0)

            logger.info(
                f"[Episode {ep:04d}] "
                f"TotalReward: {episode_reward:.2f} | "
                f"AvgReward: {episode_reward / average_step:.2f} | "
                f"WeightErr: {info['weight_error']:.2f}g | "
                f"Time: {info['total_time']:.2f}s | "
                f"SlowWeight: {slow_weight:.2f}g | "
                f"Emotion: [Curi: {cur_emotion[0]:.2f}, Cons: {cur_emotion[1]:.2f}, Anx: {cur_emotion[2]:.2f}] | "
                f"ActorLoss: {actor_loss} | CriticLoss: {critic_loss}"
            )

        if ep > 0 and ep % 100 == 0:
            duration_100 = time.time() - start_100
            logger.info(f"â±ï¸ ç¬¬ {ep-100}~{ep} å›åˆå…±è€—æ—¶ {duration_100:.2f} ç§’ï¼Œå¹³å‡ {duration_100 / 100:.2f} ç§’/å›åˆ")
            start_100 = time.time()

        if ep % 10 == 0 or ep == episodes - 1:
            torch.save(agent.get_weights(), "ddpg_emotion_agent.pth")
            logger.info(f"[Episode {ep}] æ¨¡å‹æƒé‡å·²ä¿å­˜ä¸º ddpg_emotion_agent.pth")

        agent.train_step += 1

    writer.close()
    return rewards


# # Ornstein-Uhlenbeckå™ªå£°ç”Ÿæˆå™¨ï¼ˆä¿æŒä¸å˜ï¼‰
# class OUNoise:
#     def __init__(self, size, mu=0.0, theta=0.15, sigma=0.2):
#         self.mu = mu * np.ones(size)
#         self.theta = theta
#         self.sigma = sigma
#         self.reset()
#
#     def reset(self):
#         self.state = np.copy(self.mu)
#
#     def sample(self):
#         dx = self.theta * (self.mu - self.state)
#         dx += self.sigma * np.random.randn(len(self.state))
#         self.state += dx
#         return self.state

class OUNoise:
    def __init__(self, size, mu=0.0, theta=0.15, sigma=0.6, min_sigma=0.1, decay=0.9995):
        self.size = size
        self.mu = mu * np.ones(size, dtype=np.float32)
        self.theta = theta
        self.sigma = sigma
        self.min_sigma = min_sigma
        self.decay = decay
        self.reset()

    def reset(self):
        self.state = np.copy(self.mu)

    def sample(self):
        dx = self.theta * (self.mu - self.state)
        dx += self.sigma * np.random.randn(self.size).astype(np.float32)
        self.state += dx
        return self.state

    def anneal(self):
        self.sigma = max(self.min_sigma, self.sigma * self.decay)
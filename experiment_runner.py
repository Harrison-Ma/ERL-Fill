import torch
import logging
import numpy as np
import os
import pandas as pd

from CommonInterface.ScaleTransformer import reset_actor_scaling

from WeightEnv import WeightEnv
from EmotionModule import EmotionModuleNone,EmotionModule,EmotionModuleSimple
from DifferentModules.ddpg_agent import DDPGAgent, train_ddpg
from DifferentModules.td3_agent import TD3Agent
from DifferentModules.sac_agent import SACAgent,train_sac
from DifferentModules.ERL_Fill_agent import EmotionSACAgent,train_emotion_sac
from DifferentModules.simple_emotion_sac_agent import SimpleEmotionSACAgent,train_simple_sac


def run_experiment_1(
    episodes=1000, max_steps=500, device='cuda', env_mode='sim',
    emotion_modes=['none', 'simple', 'transformer'],
    algo_list=['ddpg', 'td3', 'sac'],
    train=True, # âœ… åŠ äº†å¼€å…³
    experiment_id = 1,
    lambda_emo=0.05,
    log_prefix="experiment"
):
    """
    Emotion Ã— Algorithm å…¨ç»„åˆå®éªŒï¼Œæ”¯æŒ train/test åˆ†å¼€æ‰§è¡Œã€‚
    """
    results = []

    for algo in algo_list:
        for mode in emotion_modes:
            print(f"\n=== Running {algo.upper()} | Emotion Mode: {mode.upper()} | Train={train} ===")

            # Step 1: åˆ›å»ºç¯å¢ƒ
            env = WeightEnv()
            if env_mode == 'sim':
                env.use_offline_sim = 1
            if env_mode == 'onboard':
                env.use_offline_sim = 0
            if env_mode == 'real':
                env.use_offline_sim = 2

            # Step 2: åˆ›å»º Emotion æ¨¡å—
            if mode == 'transformer':
                emotion_module = EmotionModule(device=device)
            elif mode == 'simple':
                emotion_module = EmotionModuleSimple()
            elif mode == 'none':
                emotion_module = EmotionModuleNone()
            else:
                raise ValueError(f"Unsupported emotion mode: {mode}")

            # Step 3: åˆ›å»º Agent
            if algo == 'ddpg':
                # from WeightDemo import DDPGAgent
                agent = DDPGAgent(env, device=device)
            elif algo == 'td3':
                # from td3_module import TD3Agent
                agent = TD3Agent(env, device=device)
            elif algo == 'sac':
                # from Emotion_sac import EmotionSACAgent
                # from sac_module import SACAgent
                # from simple_emotion_sac import SimpleEmotionSACAgent
                if mode == 'none':
                    agent = SACAgent(env, device=device)
                elif mode =="simple":
                    agent = SimpleEmotionSACAgent(env, device=device)
                else:
                    agent = EmotionSACAgent(env, device=device)
            else:
                raise ValueError(f"Unsupported algorithm: {algo}")

            # Step 4: ç»‘å®šæƒ…æ„Ÿæ¨¡å—
            if hasattr(agent, 'emotion'):
                agent.emotion = emotion_module
            env.attach_agent(agent)

            # Step 5: è·¯å¾„
            if experiment_id == 2:
                model_dir = f"saved_models/{algo}_exp2_emotion_compare"
            else:
                model_dir = f"saved_models/{algo}_emotion_compare"
            os.makedirs(model_dir, exist_ok=True)
            model_path = os.path.join(model_dir, f"{algo}_{mode}_{lambda_emo}.pth")
            log_prefix = f"exp1_{algo}_emotion_{mode}_{lambda_emo}"

            if env_mode == 'sim':
                print("å·²é€‰æ‹©ä¸åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼Œå°†ä»å¤´å¼€å§‹è®­ç»ƒã€‚")
            elif env_mode == 'onboard':
                print("æ‰§è¡Œæ¿è½½è®­ç»ƒã€‚")
                pretrain_path = "pre_train_pth/sac_transformer.pth"
                # agent.load(pretrain_path)
                # print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
            elif env_mode == 'real':
                print("æ‰§è¡ŒçœŸå®è®­ç»ƒã€‚")
                pretrain_path = "pre_train_pth/AIM_v2_onboard_24900_25000.pth"
                weights = torch.load(pretrain_path, weights_only=False)
                agent.load_weights(weights)
                reset_actor_scaling(agent, env.bounds)
                print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")
            else:
                print("åŠ è½½ç°æœ‰é»˜è®¤æƒé‡ï¼Œç»§ç»­æ‰§è¡Œè®­ç»ƒã€‚")
                pretrain_path = "pre_train_pth/AIM_v2_sim_24900_25000.pth"
                agent.load(pretrain_path)
                # reset_actor_scaling(agent, env.bounds)
                print(f"æˆåŠŸåŠ è½½é¢„è®­ç»ƒæƒé‡: {pretrain_path}")

            if train:
                # === è®­ç»ƒé˜¶æ®µ ===
                print(f"ğŸš€ Start training [{algo.upper()}] with emotion mode [{mode}]")
                if algo == 'ddpg':
                    train_ddpg(env, agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
                elif algo == 'sac':
                    if mode == 'none':
                        train_sac(env, agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
                    elif mode == "simple":
                        train_simple_sac(env, agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
                    else:
                        train_emotion_sac(env, agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix, lambda_emo=lambda_emo)
                agent.save(model_path)
                print(f"âœ… Training complete. Model saved to {model_path}")

            else:
                # === æµ‹è¯•é˜¶æ®µ ===
                print(f"ğŸ” Start testing [{algo.upper()}] with emotion mode [{mode}]")
                if not os.path.exists(model_path):
                    print(f"âŒ Model not found: {model_path}. Skip.")
                    continue
                agent.load(model_path)

                total_reward = 0
                test_episodes = 10
                test_log_dir = os.path.join("logs", f"test_result_{algo}_{mode}.log")
                os.makedirs("logs", exist_ok=True)

                with open(test_log_dir, "w", encoding="utf-8") as f_log:
                    for ep in range(test_episodes):
                        state = env.reset()
                        episode_reward = 0
                        for step in range(max_steps):
                            if hasattr(agent, 'act'):
                                # === é€šç”¨åŠ¨ä½œé€‰æ‹©ï¼ˆæ ¹æ®ç®—æ³•é€‚é…ï¼‰
                                if algo in ['er_ddpg', 'ddpg', 'td3', 'emotion_td3']:
                                    action = agent.act(state, add_noise=False)

                                elif algo in ['sac', 'emotion_sac']:
                                    action = agent.act(state, deterministic=True)

                                elif algo in ['ppo']:
                                    action, _ = agent.select_action(state)

                                else:
                                    raise ValueError(f"Unsupported algorithm type: {algo}")
                            else:
                                action = agent.select_action(state)
                            action = action.squeeze() if isinstance(action, np.ndarray) else action
                            next_state, reward, done, info = env.step(action)
                            state = next_state
                            episode_reward += reward
                            if done:
                                break

                        total_reward += episode_reward

                        cur_emotion = None
                        if hasattr(agent, 'emotion') and hasattr(agent.emotion, 'get_emotion'):
                            cur_emotion = agent.emotion.get_emotion()
                            if isinstance(cur_emotion, torch.Tensor):
                                cur_emotion = cur_emotion.detach().cpu().numpy().squeeze()

                        slow_weight = info.get("action", {}).get("slow_weight", 0.0)

                        log_str = (
                            f"[Episode {ep:04d}] "
                            f"TotalReward: {episode_reward:.2f} | "
                            f"WeightErr: {info['weight_error']:.2f}g | "
                            f"Time: {info['total_time']:.2f}s | "
                            f"SlowWeight: {slow_weight:.2f}g | "
                            f"Emotion: {cur_emotion}"
                        )

                        print(f"ğŸ¯ Test Episode {ep+1}: Reward = {episode_reward:.2f}")
                        f_log.write(log_str + "\n")

                    avg_reward = total_reward / test_episodes
                    f_log.write(f"\nâœ… Avg Test Reward: {avg_reward:.2f}\n")
                    print(f"âœ… Avg Test Reward for {algo.upper()} [{mode}] = {avg_reward:.2f}")

                results.append((algo, mode, avg_reward))

    # === æ±‡æ€»æµ‹è¯•ç»“æœ ===
    if not train:
        print("\n=== ğŸ¯ Final Results: Algo Ã— Emotion Mode ===")
        for algo, mode, reward in results:
            print(f"[{algo.upper()} - {mode}] Avg Test Reward: {reward:.2f}")

    return results


def run_experiment_3(algo='er_ddpg', train=True, episodes=1000, max_steps=100, device='cuda'):
    # from WeightDemo import WeightEnv
    from baseline_experiments import (
        # build_er_ddpg_agent,
        build_ddpg_agent,
        build_td3_agent,
        build_ppo_agent,
        build_sac_agent,
        build_td3_bc_agent, build_cql_agent,
        # build_emotion_td3_agent,
        build_emotion_sac_agent,
        build_fuzzy_agent,
        build_ppol_agent, build_rls_pid_agent
    )
    from DifferentModules.td3_agent import train_td3
    from DifferentModules.td3_bc_agent import train_td3_bc
    from DifferentModules.cql_agent import train_cql
    from DifferentModules.PPOL import train_ppo_lagrangian
    from DifferentModules.rls_pidagent import train_rls_pid
    from DifferentModules.ppo_agent import train_ppo
    from DifferentModules.sac_agent import train_sac
    # from Emotion_TD3 import train_emotion_td3
    import os
    import numpy as np

    env = WeightEnv()
    algo_builders = {
        # 'er_ddpg': build_er_ddpg_agent,
        'ddpg': build_ddpg_agent,
        'td3': build_td3_agent,
        'td3_bc': build_td3_bc_agent,
        'ppo': build_ppo_agent,
        'sac': build_sac_agent,
        'cql': build_cql_agent,
        'ppol': build_ppol_agent,
        'rls_pid': build_rls_pid_agent,
        # 'emotion_td3': build_emotion_td3_agent,
        'emotion_sac': build_emotion_sac_agent,
        'fuzzy': build_fuzzy_agent  # âœ… æ·»åŠ  fuzzy agent
    }
    assert algo in algo_builders, f"Unsupported algorithm: {algo}"
    agent = algo_builders[algo](env, device=device)
    env.attach_agent(agent)

    model_dir = os.path.join("saved_models", algo)
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, f"{algo}_final.pth")
    log_prefix = f"exp3_{algo}"

    if train and algo != 'fuzzy':  # æ¨¡ç³Šæ§åˆ¶ä¸è®­ç»ƒ
        print(f"ï¿½ï¿½ Start training {algo.upper()}...")
        if algo == 'ddpg':
            from DifferentModules.ddpg_agent import train_ddpg
            train_ddpg(env, agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
            agent.save(model_path)

        elif algo == 'td3':
            train_td3(env=env, agent=agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
            agent.save(model_path)

        elif algo == 'td3_bc':
            train_td3_bc(env=env, agent=agent, episodes=episodes, max_steps=max_steps, log_prefix=log_prefix)
            agent.save(model_path)

        elif algo == 'ppo':
            train_ppo(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                      log_prefix=log_prefix, model_path=model_path)

        elif algo == 'sac':
            train_sac(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                      log_prefix=log_prefix, model_path=model_path)

        elif algo == 'cql':
            train_cql(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                      log_prefix=log_prefix, model_path=model_path)

        elif algo == 'ppol':
            train_ppo_lagrangian(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                                 log_prefix=log_prefix, model_path=model_path)

        elif algo == 'rls_pid':
            train_rls_pid(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                          log_prefix=log_prefix, model_path=model_path)

        # elif algo == 'emotion_td3':
        #     train_emotion_td3(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
        #                       log_prefix=log_prefix, model_path=model_path)

        elif algo == 'emotion_sac':
            from DifferentModules.ERL_Fill_agent import train_emotion_sac
            train_emotion_sac(env=env, agent=agent, episodes=episodes, max_steps=max_steps,
                              log_prefix=log_prefix, model_path=model_path)
        else:
            print("No such module")

        print(f"âœ… Training complete. Model saved at: {model_path}")
        avg_reward = 0
    else:
        print(f"ï¿½ï¿½ Start testing {algo.upper()}...")
        if algo != 'fuzzy' and not os.path.exists(model_path):
            raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        test_episodes = 200
        if algo != 'fuzzy':
            agent.load(model_path)
            test_episodes = 100
            print(f"âœ… æ¨¡å‹æƒé‡å·²åŠ è½½ï¼š{model_path}")

        total_reward = 0

        test_log_path = os.path.join("logs", f"test_result_{algo}.log")
        os.makedirs("logs", exist_ok=True)

        def extract_scalar(v):
            if isinstance(v, np.ndarray):
                if v.size == 1:
                    return v.item()
                elif v.ndim == 0:
                    return float(v)
                else:
                    raise ValueError(f"âŒ éæ ‡é‡ ndarray: {v} (shape={v.shape})")
            return float(v)

        with open(test_log_path, "w", encoding="utf-8") as f_log:
            for ep in range(test_episodes):
                state = env.reset()
                episode_reward = 0
                try:
                    for step in range(max_steps):
                        if algo in ['ppo', 'sac']:
                            action = agent.act(state, deterministic=True) if hasattr(agent,
                                                                                     'act') else agent.select_action(
                                state)
                        else:
                            action = agent.act(state)

                        # âœ… ä¿®å¤ fuzzy è¿”å› dict é”™è¯¯
                        if isinstance(action, dict):
                            action_dict = action
                        else:
                            action_dict = {k: extract_scalar(v) for k, v in zip(env.bounds.keys(), action)}

                        next_state, reward, done, info = env.step(action_dict)

                        state = next_state
                        episode_reward += reward
                        if done:
                            break

                    slow_weight = info["action"].get("slow_weight", 0.0)
                    log_str = (
                        f"[{algo.upper()} | Episode {ep:04d}] "
                        f"TotalReward: {episode_reward:.2f} | "
                        f"WeightErr: {info['weight_error']:.2f}g | "
                        f"Time: {info['total_time']:.2f}s | "
                        f"SlowWeight: {slow_weight:.2f}g"
                    )
                    print(f"ï¿½ï¿½ Test Episode {ep + 1}: {episode_reward:.2f}")
                    f_log.write(log_str + "\n")

                except Exception as e:
                    error_msg = f"[{algo.upper()} | Episode {ep:04d}] âŒ æµ‹è¯•å‡ºé”™: {e}"
                    print(error_msg)
                    f_log.write(error_msg + "\n")

                total_reward += episode_reward

            avg_reward = total_reward / test_episodes
            summary = f"\nâœ… Avg Test Reward for {algo.upper()}: {avg_reward:.2f}\n"
            print(summary)
            f_log.write(summary)

    return model_path, avg_reward


if __name__ == "__main__":
    # === å…¨å±€é…ç½® ===
    device = 'cuda'
    train_mode = True        # âœ… True å¼€å§‹è®­ç»ƒï¼ŒFalse å¼€å§‹æµ‹è¯•
    experiment_id = 1        # âœ… è®¾ç½®ä¸º 1ã€2ã€3ã€4 é€‰æ‹©å®éªŒç»„
    # selected_algo = 'er_ddpg'  # å®éªŒ3ã€4ä¸“ç”¨
    episodes = 5
    max_steps = 50
    test_episodes = 10
    # # selected_algo = 'ddpg'
    # # selected_algo = 'td3'
    # # selected_algo = 'ppo'
    # selected_algo = 'sac'
    use_offline_sim = 1 #1-é‡‡æ ·ç¦»çº¿ä»¿çœŸï¼Œ0-é‡‡ç”¨æ¿è½½ä»¿çœŸï¼Œ2-çœŸå®ç³»ç»Ÿè®­ç»ƒ

    # === å®éªŒä¸€ï¼šæƒ…æ„Ÿæœºåˆ¶å¯¹ç…§ç»„ ===
    # === å®éªŒä¸€ï¼šæƒ…æ„Ÿæœºåˆ¶å¯¹ç…§ç»„ï¼ˆ5ç»„å®éªŒï¼‰ ===
    if experiment_id == 1:
        emotion_modes = ['none', 'simple', 'transformer']  # Baselineã€Simpleã€Transformer
        algo_list = ['sac']  # æœ¬å®éªŒåªè·‘SAC
        results = []

        # === å®šä¹‰5ç»„å®éªŒé…ç½® ===
        experiment_configs = [
            {'name': 'Baseline', 'mode': 'none', 'lambda_emo': 0.0},
            {'name': 'Simple', 'mode': 'simple', 'lambda_emo': 0.05},
            {'name': 'Transformer', 'mode': 'transformer', 'lambda_emo': 0.0},
            {'name': 'Transformer', 'mode': 'transformer', 'lambda_emo': 0.01},
            {'name': 'Transformer-High', 'mode': 'transformer', 'lambda_emo': 0.1},
            {'name': 'Transformer-Low', 'mode': 'transformer', 'lambda_emo': 0.2},
        ]

        for config in experiment_configs:
            mode = config['mode']
            lambda_emo = config['lambda_emo']
            group_name = config['name']

            print(f"\n=== Running Group: {group_name} | Mode: {mode} | Î»_emo: {lambda_emo} ===")

            # === ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒ ===
            _ = run_experiment_1(
                episodes=episodes,
                max_steps=max_steps,
                device=device,
                env_mode='sim',
                emotion_modes=[mode],
                algo_list=algo_list,
                train=True,
                lambda_emo=lambda_emo,
                log_prefix=f"{group_name}_train"
            )

            # === ç¬¬äºŒæ­¥ï¼šæµ‹è¯• ===
            test_results = run_experiment_1(
                episodes=test_episodes,
                max_steps=max_steps,
                device=device,
                env_mode='sim',
                emotion_modes=[mode],
                algo_list=algo_list,
                train=False,
                lambda_emo=lambda_emo,
                log_prefix=f"{group_name}_test"
            )

            # ç»“æœä¿å­˜
            for algo, mode_result, avg_reward in test_results:
                results.append((group_name, algo, mode_result, avg_reward))

        # === æœ€ç»ˆå®éªŒç»“æœè¾“å‡º ===
        print("\n=== âœ… å®éªŒä¸€ï¼ˆEmotion Mechanism Ablationï¼‰5ç»„å¯¹ç…§ç»“æœ ===")
        for group, algo, mode, reward in results:
            print(f"[Group: {group} | Algo: {algo.upper()} | Mode: {mode}] â†’ AvgTestReward: {reward:.2f}")

    # # === å®éªŒäºŒï¼šåˆ†é˜¶æ®µé¢„è®­ç»ƒç»“æ„å¯¹æ¯” ===
    # elif experiment_id == 2:
    #     print("\n=== Running Multi-Stage Pretraining Evaluation ===")
    #     run_experiment_2(device=device, train=train_mode, use_off_sim=use_offline_sim)

    # === å®éªŒä¸‰ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯” ===
    elif experiment_id == 3:
        print(f"\n=== Running Algorithm Comparison for All Methods ===")
        algo_list = ['er_ddpg', 'ddpg', 'td3', 'ppo', 'sac','emotion_td3','emotion_sac']
        # algo_list = ['emotion_sac','sac']
        # algo_list = ['emotion_sac']
        results = []

        for algo in algo_list:
            print(f"\n>>> ğŸš€ Start {algo.upper()} Training & Testing")
            model_path, test_reward = run_experiment_3(
                algo=algo,
                train=train_mode,
                episodes=episodes,
                max_steps=max_steps,
                device=device
            )
            results.append((algo, test_reward))

        print("\n=== âœ… å®éªŒä¸‰ç»“æœå¯¹æ¯” ===")
        for algo, reward in results:
            print(f"[{algo.upper()}] å¹³å‡æµ‹è¯•å¥–åŠ±: {reward:.2f}")

    # === å®éªŒä¸‰ï¼šå¼ºåŒ–å­¦ä¹ ç®—æ³•å¯¹æ¯” ===
    elif experiment_id == 3:
        print(f"\n=== Running Algorithm Comparison for All Methods ===")
        # algo_list = ['er_ddpg', 'ddpg', 'td3','td3_bc','ppo', 'sac', 'cql', 'ppol', 'rls_pid', 'emotion_td3','emotion_sac', 'fuzzy']  # âœ… åŒ…å«æ¨¡ç³Šæ§åˆ¶
        algo_list = ['td3','sac','ppo','ddpg','td3_bc','rls_pid', 'cql','emotion_sac', 'fuzzy']  # âœ… åŒ…å«æ¨¡ç³Šæ§åˆ¶
        results = []

        for algo in algo_list:
            print(f"\n>>> ïš€ Start {algo.upper()} Training & Testing")
            model_path, test_reward = run_experiment_3(
                algo=algo,
                train=train_mode,
                episodes=episodes,
                max_steps=max_steps,
                device=device
            )
            results.append((algo, test_reward))

        print("\n=== âœ… å®éªŒä¸‰ç»“æœå¯¹æ¯” ===")
        for algo, reward in results:
            print(f"[{algo.upper()}] å¹³å‡æµ‹è¯•å¥–åŠ±: {reward:.2f}")

    # # === å®éªŒå››ï¼šå¤šå·¥å†µå¯¹æ¯”å®éªŒ ===
    # elif experiment_id == 4:
    #     print(f"\n=== Running Multi-Condition Comparison Experiment ===")
    #
    #     # é€‰æ‹©è¦è·‘çš„ç®—æ³•
    #     algo_list = ['emotion_sac']  # å¯ä»¥ä»»é€‰
    #
    #     for algo in algo_list:
    #         print(f"\n=== ğŸš€ Running {algo.upper()} for Multi-Condition ===")
    #         run_experiment_4(
    #             train=True,  # True=è®­ç»ƒ+æµ‹è¯•ï¼ŒFalse=åªæµ‹è¯•
    #             episodes=episodes,
    #             max_steps=max_steps,
    #             device=device,
    #             algo=algo  # âœ… ä¼ å…¥æŒ‡å®šç®—æ³•
    #         )

    else:
        print("âŒ Unsupported experiment ID. Please set experiment_id = 1, 2, 3, or 4.")